So i thought I made something like this before but I'm not seeing it.

I think, however, I can use 
https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-iterators-for-grouped-data
Groups, and threshold to create a group.

The most test like group

So it sounds like the first thing to do is to train an estimator that can predictt
the liklihood of something being in the test set or not

Then given a data set, threshold data
return https://scikit-learn.org/stable/modules/cross_validation.html#predefined-fold-splits-validation-sets
predefined split with 0 for the validation set dervied from the dataset
see: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.PredefinedSplit.html#sklearn.model_selection.PredefinedSplit

Then we have a cross validator that can be used, reused based on threshold.

should sketch out responsibilties and then decompose with SOLID
https://scotch.io/bar-talk/s-o-l-i-d-the-first-five-principles-of-object-oriented-design
--

so I realized later that I could instead break up seperation of concerns and do the following:
* make a class that accepts a classifier, predicts probability and recieves a dataset, 
with test examples marked. On fit it learns to predict if an example if test or not
* make a scorer that accepts a y predicted, y actual and probability of being test instance, returns a scorer

the issue is thst the scorer does not accept other data

estimators have a scoring function, https://github.com/scikit-learn/scikit-learn/blob/1495f6924/sklearn/model_selection/_search.py#L829#L849

let's see if they have access to X (they must?)

okay, see: https://github.com/scikit-learn/scikit-learn/blob/1495f6924/sklearn/base.py#L332
looks like the cleanest solution is to make a scorer for a estimator mixin and 
inheriet the base estimator. From there all CV, grid,etc soln shoudl work
--
So I should probably make a Transformer that learns to label test, train instances
pass the transformer as a mixin to classifer mixin, which also takes a custom scorer. 
I'm nervous that the scorer depends on the transformer
also see: https://github.com/heykarimoff/solid.python

so, from the above the scorer should have an abstract interface to the transformer, ah but it 
might not use the scorer except for the predict function, which violates seperation

I think you can use a mixin, letme double check aginast SRP
is interesting: https://punq.readthedocs.io/en/latest/
--

okay so I think the cleanest way to do this is start with the Transformer

-- 

have util functions working, verified the basic structure of the Adversarial labeller works

The AdversarialLabeller, using LR mixin, doesn't seem to also have .solver
like LR does.

--

Okay, been a while before I put notes back in htere.
I've gotten things set up mostly better, I confused myself with the wrong target
variable. But ther ear some fixes that are needed:

My sample weights are not normalized to add up to one, that needs to happen. 
I am not sure if it'll affect how the random forest weights its learning though.

I'm seeing a roughly 10 point different between my hold out and Kaggle's hold out
Yet I'm only getting AUC for predicting if a sample is in test or not

The examples I've seen of adversarial testing 
take those scored as 1 only: https://www.kaggle.com/kevinbonnes/adversarial-validation

I am curious about modifying scoring while training 
    like if predicted 1, then weight as 1
    else weight as 0.5

```
    mask  = sample_weights == 1
    print(
        accuracy_score(labels[rf_test_df.index][mask].values,
                       rf2.predict(rf_test_df[mask].values)
        )
    )
```

GIves 0.68 on hold out, which is a bit closer to 0.64
---
Trained with test label detecor AUC of 0.56, however, hold out score is 0.82
actual score is 0.60 ... so the extra trees memorized the test distribution a tad too much

There is something slightly different, which might affect things:
* in practice they mark the test examples as 1 and all others as 0
* They then train on the whole thing, use CV validation, they do not randomly sample
like I have. 
* Their AUC to validated result vs actual is much closer

todo: do straight full data set training, see if that makes the actual vs validated
results much closer. This will allow us to then make a scorer
that can do validated scoring and drive model parameter selection from the 
fold, etc.
--

rewrote some parts, focused on cross validation, saw a lot of variation.
have an issue with classifiers want to predict 1 a lot. Currently actual
is target with 0.68 precision, almost full recall, on hold out set.

going to try with only balanced, balanced subsample approaches
that works a bit better:

Validation Accuracy: 0.61
              precision    recall  f1-score   support

           0       0.33      0.19      0.24       363
           1       0.67      0.81      0.74       750

    accuracy                           0.61      1113
   macro avg       0.50      0.50      0.49      1113
weighted avg       0.56      0.61      0.57      1113

note: https://github.com/scikit-learn-contrib/imbalanced-learn
note: https://arxiv.org/abs/1804.07155

using a lower ntrees start, entropy

Accuracy: 0.61 (+/- 0.07)
Validation Accuracy: 0.61
              precision    recall  f1-score   support

           0       0.33      0.19      0.24       363
           1       0.68      0.82      0.74       750

    accuracy                           0.61      1113
   macro avg       0.51      0.50      0.49      1113
weighted avg       0.56      0.61      0.58      1113

kNN GIves
Accuracy: 0.61 (+/- 0.07)
Validation Accuracy: 0.66
              precision    recall  f1-score   support

           0       0.37      0.06      0.11       363
           1       0.68      0.95      0.79       750
    accuracy                           0.66      1113
   macro avg       0.52      0.51      0.45      1113
weighted avg       0.58      0.66      0.57      1113

the arxiv paper above inspired me to try kNN
---
imported imbalnced sklearn
with balanced random forest we getting
Validation Accuracy: 0.51
              precision    recall  f1-score   support

           0       0.34      0.56      0.43       363
           1       0.69      0.49      0.57       750

    accuracy                           0.51      1113
   macro avg       0.52      0.52      0.50      1113
weighted avg       0.58      0.51      0.52      1113

(note the Accuracy scores above were printing old score info, hence being the same; i think)

okay, so what I really want is to essentially make a labeller that is a pretty 
much always correct when the predicted probabilty is a 1. I don't particularly wnat to
muck around too much so I can, instead, attempt to maximize the F1 score of both
in test and out of test predictions.

In a sense, because imbalance respects class imbalance, I can probably use
maximizing the in test class F1 score and it won't go off the rails and say
everythign is a 1 (gettin ga accuracy of 0.67 but 0 for out of class)

so What I should do here to close out this fork of work is make an parameter _search
imbalanced RUSBoost classifier and leave it at that.

The final step will be to add more text features and then, finally test against Kaggle
results from a holdout validation step
--

okay do a full rusboost run (that was too long), gotten

Validation Accuracy: 0.34
              precision    recall  f1-score   support

           0       0.32      0.90      0.47       363
           1       0.58      0.07      0.12       750

    accuracy                           0.34      1113
   macro avg       0.45      0.48      0.29      1113
weighted avg       0.50      0.34      0.23      1113

Given that that this is a binary problem, it would seem that I could flip the deicsions and get 
a validation accuracy of 0.66?

Yeah, this is funny:

In [93]:     print( 
    ...:         "Validation Accuracy: %0.2f" % ( 
    ...:             accuracy_score( 
    ...:                 labels[test_df.index], 
    ...:                 adversarial_labeller.predict( 
    ...:                     test_df.values 
    ...:                 ) ^ 1 
    ...:             ) 
    ...:         ) 
    ...:     )                                                                          
Validation Accuracy: 0.66
(note the ^ 1 for bit flipping)
and

In [94]:     print( 
    ...:         classification_report( 
    ...:             y_true= labels[test_df.index], 
    ...:             y_pred= adversarial_labeller.predict( 
    ...:                         test_df.values 
    ...:                     ) ^ 1 
    ...:         ) 
    ...:     )                                                                          
              precision    recall  f1-score   support

           0       0.42      0.10      0.16       363
           1       0.68      0.93      0.79       750

    accuracy                           0.66      1113
   macro avg       0.55      0.52      0.47      1113
weighted avg       0.59      0.66      0.58      1113

k, I've taken this about as far as I should go. 

I think at this point I'm at the essential question (and probably over fitting
to one data set) of whether for adversarial validation is better having
A) a higher accuracy (validating against those prediced to be 1) or
B) a higher predicted probably (e.g. prob = 1)

I think the answer is strongly influenced by the data drift present since (A) 
brings in non test like samples for validation. The score could also be weighted by
sample confidence but the RUSBooster seems to give one of two confidences and that's it.

okay so what I shoudl do is wrap this part up

The rusbooster appears to be best, ignoring the predicited probability

Okay, I think I'm done at this part, just need to capture the code in main() 
and make into a function to build out the labeller. Actually I think I'm done,
running fit and then maiximize adversarl validation seperates concerns and isn'tad
a big lift

okay so the next steps next week (if I don't abandon this work due to a wave of new 
class work) is to:
* holdout some data
* train a normal random forest on the remaining data
* label the hold out as in test or out of test
* get teh accuracy score on the in test hold out data using the RF and known label
* that score should be +/- 3 points within the Kaggle result (ideally)

If that doesn't work then that might suggest that the predict probabilities are more important
(although I feel liek might've failed way back when?), then figure out an approaches
that maximizes predicted probabilty as well as gets punished for predicting all 1s
--

okay, so in a way I have an inital success
adverarial labelling predicts 0.67
Kaggle gave 0.66985

The thing is my adverarial labeller, in this case, was trained with a minimal set of checking to speed
up dev time. What I watn to do is switch to a random CV, get a good version and try again.

the other thing that bothers me is how complex this is to set up, i kinda wnat a one function call
you give it the data, a classifier and it'll return an adversarial validation score.

but I defintely got where i wanted today, need to test against a well cross validated parameter set
and verify it still works as expected.

then i should try it against a different type of classifier and then rearchitect slightly for ease of use
we can already drop it into sklearn fairly easily.
--

So I did some deeper searching, around sklearn pipelines, and it turns out there's a
cross validator that accepts a scoring function, here I would not use the make_scorer
see sklearn 3.3.1.3: Implementing your own scoring object.

This is what teh adversial labeller needs to be written as, then labels X
and only evaluates on those X labelled as test.
--

so the adverarial part would need it's own transformer I think (or none)
it actaully may need a mini transform pipeline
so I think I would actually train the a...

okay, so you would make a factory of sorts that trans, generates
a pipeline that ends in the adversarial labeller and can be dropped in
the factory needs the rigth input, any pipeline components as well and then
append the trained adversarial predictor to the end of it. Kinda annoying
but this will make it drop in easy.
---
okay, got basic inital for factory, let's commit
--
so now I want to get the get_best_paramet working, then make it actually return a fitted
adverarial labeller (that calls the pipeline on it too)
-

So the pipeline object works, needs to be called with
In [89]:     labelled_test_mask =\ 
    ...:         fit.predict( 
    ...:             data['validate']['data'].drop("PassengerId", axis="columns") 
    ...:         ) == 1    
   
which is kinda weird but it's a function of how I set up the preprocessor
and what data I fed it

with the above we get
In [93]:     # and use that test labeled set as a validation set, compare to Kaggle 
    ...:     accuracy_score( 
    ...:         y_true= data["validate"]["labels"][labelled_test_mask].values, 
    ...:         y_pred= clf.predict( 
    ...:           data["validate"]["data"][labelled_test_mask].values 
    ...:         ) 
    ...:     ) 
    ...:                                                                                                                                  
Out[93]: 0.6940298507462687

and I noticed that it says all labels are true on the hold out set
and we already predicted against that and got 0.67, so I will skip that
verification for now (?)

Goign to callit, next steps are to:

* test against / add in scorer for custom cross validation scorer,
see: https://scikit-learn.org/stable/modules/model_evaluation.html#implementing-your-own-scoring-object
for gory details

* rewrite main, the example file to use the piepeline, provide 
fixture specific test files (should make this a test I suppose), move
to unit test, but add as to read me as a simple example

* verify adversarial accuracy is mostly spot on (w/in -/+ 1 point of accuracy)

* modify so it's pip installable
--
okay so have in test the class that produces a labeller
I now need to read in data, pass as a scorer to a 
CV parameter fit.
--
have a sketch for the cross val, as a scorer
I'm seeing different validation results, which all should
be the same, I suspect the randomizedCV search is starting
along different paths, i thought i passed in the same seed value.
need to double check this

Scorer was also failing, i forget the exact message but I removed teh 
wrapper (which was in the example) and it seems to work. I now
see an issue with one of transformers that expects a data frame,
we now see:
---
  File "/Users/kimrobinson/.local/share/virtualenvs/adversarial_validatior-jlSiO2el/lib/python3.7/site-packages/imblearn/pipeline.py", line 347, in predict
    Xt = transform.transform(Xt)
  File "/Users/kimrobinson/.local/share/virtualenvs/adversarial_validatior-jlSiO2el/lib/python3.7/site-packages/sklearn/preprocessing/_function_transformer.py", line 153, in transform
    return self._transform(X, func=self.func, kw_args=self.kw_args)
  File "/Users/kimrobinson/.local/share/virtualenvs/adversarial_validatior-jlSiO2el/lib/python3.7/site-packages/sklearn/preprocessing/_function_transformer.py", line 179, in _transform
    return func(X, **(kw_args if kw_args else {}))
  File "/Users/kimrobinson/Desktop/Kwame/open_source/adversarial_validatior/tests/test_adversarial_labeller.py", line 45, in keep_numeric_columns_only_replace_na
    return df.select_dtypes(include='number')\
AttributeError: 'numpy.ndarray' object has no attribute 'select_dtypes'
---

which suggests that I update the transformer to take in a numpy matrix instead
so that it'll work on both. I can do this with a wrapper inside teh function

--
added some code to convert a numpy matrix to pandas dataframe, drop keep_numeric_columns_only_replace_na

note when intallign in a new directory you must run
`pipenv shell && pipenv install && pipenv install --dev`
since --dev has pytest, otherwise it'll call the system version and try to run yourp
python3 (e.g. python3.5) and you wil lget f string format related errors since it doesn't support f strings

```
(adversarial_validator) kwame@Puget-168695:$ pytest --version
This is pytest version 3.5.0, imported from /home/kwame/.local/lib/python3.5/site-packages/pytest.py
(adversarial_validator) kwame@Puget-168695:$ python pytest --version
python: can't open file 'pytest': [Errno 2] No such file or directory
(adversarial_validator) kwame@Puget-168695:$ python -m pytest --version
This is pytest version 5.2.0, imported from /home/kwame/.local/share/virtualenvs/adversarial_validator-OGseoHRw/lib/python3.7/site-packages/pytest.py
```

okay, then you hae to run with `python -m pytest --pdb -s` >:|
-
Got the labeller workign int he cv loop, I can't trigger the prediction failure
but the validation accuracy results have differnt 0, 1 precisions so I know
that the RandomizedSearchCV or somethign is different somethign idfferent
but i also pass in a 1 for random state so I'm not where this is coming from.
--
Okay so next steps are to include an example with fake data in the README
Make it pip installable

for RandomizedSearchCV, see this note
```
Note that before SciPy 0.16, the scipy.stats.distributions do not
accept a custom RNG instance and always use the singleton RNG from
numpy.random. Hence setting random_state will not guarantee a deterministic
iteration whenever scipy.stats distributions are used to define the parameter
search space
```

but

```
Python 3.7.2 (default, Jan 11 2019, 21:31:15) 
[GCC 5.4.0 20160609] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> import scipy
>>> scipy.__version__
'1.3.1'
```
so not sure what's up there
---
x okay, need to test out pip installable
got somethign basic up, need to remember to update when done with the
other stuff

hmm, so I'm almost done, I need to write a quite example of use
so i think i can make data drfit by generating a two features classification 
problem w sklearn and then shift the mean slightly and mark those test

okay, i got to go to bed here but:
a) use sample_generator to create read_concat_and_label_test_train_data, with a
train and test set, where test set is train set + 1 sd or something <-- data drift
mark with 0 or 1 as appropriate. hold out the test set answers
b) Call get variables and labels, pass to factory for adver. labeller
c) create a classifer
d)  make a split of training with a portion reserved for test
  classifer, labeller pipeline
    to cross_val_score

    labeller pipeline should pick from those in test that look like
    the real test

    get overall score (too optimistic)
    get adversarial labelled score

    compare to true test, should be similar
--
scratch

import numpy as np
import pandas as pd
from sklearn.datasets.samples_generator import make_blobs
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from adversarial_labeller import AdversarialLabelerFactory, Scorer

# Our blob data generation parameters for this example
number_of_samples = 1000
number_of_test_samples = 300

# Generate 1d blob data and label a portion as test data
# ... 1d blob data can be visualized as a rug plot
variables, labels = \
  make_blobs(
    n_samples=number_of_samples,
    centers=2,
    n_features=1,
    random_state=0
)

df = pd.DataFrame(
  {
    'independent_variable':variables.flatten(),
    'dependent_variable': labels,
    'label': 0  #  default to train data
  }
)
test_indices = df.index[-number_of_test_samples:]
train_indices = df.index[:-number_of_test_samples]

df.loc[test_indices,'label'] = 1  # ... now we mark instances that are test data

# Now perturb the test samples to simulate data drift/different test distribution
df.loc[test_indices, "independent_variable"] +=\
  np.std(df.independent_variable)

# ... now we have an example of data drift that adversarial labeling 
# can be used for this example

features_for_labeller = df.independent_variable
labels_for_labeller = df.label

pipeline, flip_binary_predictions =\
    AdversarialLabelerFactory(
        features = features_for_labeller,
        labels = labels_for_labeller,
        run_pipeline = False
    ).fit_with_best_params()

scorer = Scorer(the_scorer=pipeline,
                flip_binary_predictions=flip_binary_predictions)

# Now we evaluate a classifer on training data only, but using
# our fancy adversarial labeller

# ... sklearn wants firmly defined shapes
clf_adver = RandomForestClassifier(n_estimators=100, random_state=1)
_X = df.loc[train_indices]\
       .independent_variable\
       .values\
       .reshape(-1,1)
adversarial_scores =\
    cross_val_score(
        X=_X,
        y=df.loc[train_indices].dependent_variable,
        estimator=clf_adver,
        scoring=scorer.grade,
        cv=5,
        n_jobs=1,
        verbose=2)

# ... and we get ~ xyz
average_adversarial_score =\
    np.array(adversarial_scores).mean()

# ... let's see how this compares with normal cross validation
clf = RandomForestClassifier(n_estimators=100, random_state=1)
scores =\
    cross_val_score(
        X=_X,
        y=df.loc[train_indices].dependent_variable,
        estimator=clf,
        cv=5,
        n_jobs=1,
        verbose=2)

average_score =\
    np.array(scores).mean()

# now let's see how this compares with the actual test score
clf_all = RandomForestClassifier(n_estimators=100, random_state=1)
_X_test = df.loc[test_indices]\
            .independent_variable\
            .values\
            .reshape(-1,1)
clf_all.fit(_X_test,
            df.loc[test_indices].dependent_variable)

actual_score =\
  accuracy_score(
    clf_all.predict(_X_test),
    df.loc[test_indices].dependent_variable
  )

adversarial_result = abs(average_adversarial_score - actual_score)
print(f"... adversarial labelled validation was {adversarial_result} percent different than actual.")

cross_val_result = abs(average_score - actual_score)
print(f"... adversarial labelled validation was {cross_val_result:.2f} percent different than actual.")

# adversarial model validation gives us a better result 
# here under this data drift
--
#####################################################
--
Will continue to modify the above as needed

but inserted some reshaping logic, imbalanced learn doens't like
feature matrices of (1,), (actually, sklearn might not, since it's 
undeterminate; could be columns or rows). Need to circle abck and
verify that the test fixture still works (shoudl sicne it uses a larger
feature array, won't trigger that reshape logic; but I do stop values
from being passed down)

anwyay, so the pipeline now runs so the rest of this example
should be fairly easy (assuming it improves over the standard approach)
---
okay, am able to reproduce to where I left off on a totally new 
system so I think things are generally reproducible
I don't like how there's not enough time for the RandomizedSearchCV
to find a better solution than predicting all 1's, want to see
if I can bump the time given

yeah, tweaked it to do 200 n_iters (sample of the distribution)
and we're at 75 accuracy with much better F1 score, yay

k, i'll follow up tmmrw as I can
----