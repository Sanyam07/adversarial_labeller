So i thought I made something like this before but I'm not seeing it.

I think, however, I can use 
https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-iterators-for-grouped-data
Groups, and threshold to create a group.

The most test like group

So it sounds like the first thing to do is to train an estimator that can predictt
the liklihood of something being in the test set or not

Then given a data set, threshold data
return https://scikit-learn.org/stable/modules/cross_validation.html#predefined-fold-splits-validation-sets
predefined split with 0 for the validation set dervied from the dataset
see: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.PredefinedSplit.html#sklearn.model_selection.PredefinedSplit

Then we have a cross validator that can be used, reused based on threshold.

should sketch out responsibilties and then decompose with SOLID
https://scotch.io/bar-talk/s-o-l-i-d-the-first-five-principles-of-object-oriented-design
--

so I realized later that I could instead break up seperation of concerns and do the following:
* make a class that accepts a classifier, predicts probability and recieves a dataset, 
with test examples marked. On fit it learns to predict if an example if test or not
* make a scorer that accepts a y predicted, y actual and probability of being test instance, returns a scorer

the issue is thst the scorer does not accept other data

estimators have a scoring function, https://github.com/scikit-learn/scikit-learn/blob/1495f6924/sklearn/model_selection/_search.py#L829#L849

let's see if they have access to X (they must?)

okay, see: https://github.com/scikit-learn/scikit-learn/blob/1495f6924/sklearn/base.py#L332
looks like the cleanest solution is to make a scorer for a estimator mixin and 
inheriet the base estimator. From there all CV, grid,etc soln shoudl work
--
So I should probably make a Transformer that learns to label test, train instances
pass the transformer as a mixin to classifer mixin, which also takes a custom scorer. 
I'm nervous that the scorer depends on the transformer
also see: https://github.com/heykarimoff/solid.python

so, from the above the scorer should have an abstract interface to the transformer, ah but it 
might not use the scorer except for the predict function, which violates seperation

I think you can use a mixin, letme double check aginast SRP
is interesting: https://punq.readthedocs.io/en/latest/
--

okay so I think the cleanest way to do this is start with the Transformer

-- 

have util functions working, verified the basic structure of the Adversarial labeller works

The AdversarialLabeller, using LR mixin, doesn't seem to also have .solver
like LR does.

--

Okay, been a while before I put notes back in htere.
I've gotten things set up mostly better, I confused myself with the wrong target
variable. But ther ear some fixes that are needed:

My sample weights are not normalized to add up to one, that needs to happen. 
I am not sure if it'll affect how the random forest weights its learning though.

I'm seeing a roughly 10 point different between my hold out and Kaggle's hold out
Yet I'm only getting AUC for predicting if a sample is in test or not

The examples I've seen of adversarial testing 
take those scored as 1 only: https://www.kaggle.com/kevinbonnes/adversarial-validation

I am curious about modifying scoring while training 
    like if predicted 1, then weight as 1
    else weight as 0.5

```
    mask  = sample_weights == 1
    print(
        accuracy_score(labels[rf_test_df.index][mask].values,
                       rf2.predict(rf_test_df[mask].values)
        )
    )
```

GIves 0.68 on hold out, which is a bit closer to 0.64
---
Trained with test label detecor AUC of 0.56, however, hold out score is 0.82
actual score is 0.60 ... so the extra trees memorized the test distribution a tad too much

There is something slightly different, which might affect things:
* in practice they mark the test examples as 1 and all others as 0
* They then train on the whole thing, use CV validation, they do not randomly sample
like I have. 
* Their AUC to validated result vs actual is much closer

todo: do straight full data set training, see if that makes the actual vs validated
results much closer. This will allow us to then make a scorer
that can do validated scoring and drive model parameter selection from the 
fold, etc.
--

rewrote some parts, focused on cross validation, saw a lot of variation.
have an issue with classifiers want to predict 1 a lot. Currently actual
is target with 0.68 precision, almost full recall, on hold out set.

going to try with only balanced, balanced subsample approaches
that works a bit better:

Validation Accuracy: 0.61
              precision    recall  f1-score   support

           0       0.33      0.19      0.24       363
           1       0.67      0.81      0.74       750

    accuracy                           0.61      1113
   macro avg       0.50      0.50      0.49      1113
weighted avg       0.56      0.61      0.57      1113

note: https://github.com/scikit-learn-contrib/imbalanced-learn
note: https://arxiv.org/abs/1804.07155

using a lower ntrees start, entropy

Accuracy: 0.61 (+/- 0.07)
Validation Accuracy: 0.61
              precision    recall  f1-score   support

           0       0.33      0.19      0.24       363
           1       0.68      0.82      0.74       750

    accuracy                           0.61      1113
   macro avg       0.51      0.50      0.49      1113
weighted avg       0.56      0.61      0.58      1113

kNN GIves
Accuracy: 0.61 (+/- 0.07)
Validation Accuracy: 0.66
              precision    recall  f1-score   support

           0       0.37      0.06      0.11       363
           1       0.68      0.95      0.79       750
    accuracy                           0.66      1113
   macro avg       0.52      0.51      0.45      1113
weighted avg       0.58      0.66      0.57      1113

the arxiv paper above inspired me to try kNN
---
imported imbalnced sklearn
with balanced random forest we getting
Validation Accuracy: 0.51
              precision    recall  f1-score   support

           0       0.34      0.56      0.43       363
           1       0.69      0.49      0.57       750

    accuracy                           0.51      1113
   macro avg       0.52      0.52      0.50      1113
weighted avg       0.58      0.51      0.52      1113

(note the Accuracy scores above were printing old score info, hence being the same; i think)

okay, so what I really want is to essentially make a labeller that is a pretty 
much always correct when the predicted probabilty is a 1. I don't particularly wnat to
muck around too much so I can, instead, attempt to maximize the F1 score of both
in test and out of test predictions.

In a sense, because imbalance respects class imbalance, I can probably use
maximizing the in test class F1 score and it won't go off the rails and say
everythign is a 1 (gettin ga accuracy of 0.67 but 0 for out of class)

so What I should do here to close out this fork of work is make an parameter _search
imbalanced RUSBoost classifier and leave it at that.

The final step will be to add more text features and then, finally test against Kaggle
results from a holdout validation step
--

okay do a full rusboost run (that was too long), gotten

Validation Accuracy: 0.34
              precision    recall  f1-score   support

           0       0.32      0.90      0.47       363
           1       0.58      0.07      0.12       750

    accuracy                           0.34      1113
   macro avg       0.45      0.48      0.29      1113
weighted avg       0.50      0.34      0.23      1113

Given that that this is a binary problem, it would seem that I could flip the deicsions and get 
a validation accuracy of 0.66?

Yeah, this is funny:

In [93]:     print( 
    ...:         "Validation Accuracy: %0.2f" % ( 
    ...:             accuracy_score( 
    ...:                 labels[test_df.index], 
    ...:                 adversarial_labeller.predict( 
    ...:                     test_df.values 
    ...:                 ) ^ 1 
    ...:             ) 
    ...:         ) 
    ...:     )                                                                          
Validation Accuracy: 0.66
(note the ^ 1 for bit flipping)
and

In [94]:     print( 
    ...:         classification_report( 
    ...:             y_true= labels[test_df.index], 
    ...:             y_pred= adversarial_labeller.predict( 
    ...:                         test_df.values 
    ...:                     ) ^ 1 
    ...:         ) 
    ...:     )                                                                          
              precision    recall  f1-score   support

           0       0.42      0.10      0.16       363
           1       0.68      0.93      0.79       750

    accuracy                           0.66      1113
   macro avg       0.55      0.52      0.47      1113
weighted avg       0.59      0.66      0.58      1113

k, I've taken this about as far as I should go. 

I think at this point I'm at the essential question (and probably over fitting
to one data set) of whether for adversarial validation is better having
A) a higher accuracy (validating against those prediced to be 1) or
B) a higher predicted probably (e.g. prob = 1)

I think the answer is strongly influenced by the data drift present since (A) 
brings in non test like samples for validation. The score could also be weighted by
sample confidence but the RUSBooster seems to give one of two confidences and that's it.

okay so what I shoudl do is wrap this part up

The rusbooster appears to be best, ignoring the predicited probability

Okay, I think I'm done at this part, just need to capture the code in main() 
and make into a function to build out the labeller. Actually I think I'm done,
running fit and then maiximize adversarl validation seperates concerns and isn'tad
a big lift

okay so the next steps next week (if I don't abandon this work due to a wave of new 
class work) is to:
* holdout some data
* train a normal random forest on the remaining data
* label the hold out as in test or out of test
* get teh accuracy score on the in test hold out data using the RF and known label
* that score should be +/- 3 points within the Kaggle result (ideally)

If that doesn't work then that might suggest that the predict probabilities are more important
(although I feel liek might've failed way back when?), then figure out an approaches
that maximizes predicted probabilty as well as gets punished for predicting all 1s
--

okay, so in a way I have an inital success
adverarial labelling predicts 0.67
Kaggle gave 0.66985

The thing is my adverarial labeller, in this case, was trained with a minimal set of checking to speed
up dev time. What I watn to do is switch to a random CV, get a good version and try again.

the other thing that bothers me is how complex this is to set up, i kinda wnat a one function call
you give it the data, a classifier and it'll return an adversarial validation score.

but I defintely got where i wanted today, need to test against a well cross validated parameter set
and verify it still works as expected.

then i should try it against a different type of classifier and then rearchitect slightly for ease of use
we can already drop it into sklearn fairly easily.
--

So I did some deeper searching, around sklearn pipelines, and it turns out there's a
cross validator that accepts a scoring function, here I would not use the make_scorer
see sklearn 3.3.1.3: Implementing your own scoring object.

This is what teh adversial labeller needs to be written as, then labels X
and only evaluates on those X labelled as test.
--
